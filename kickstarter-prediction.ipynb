{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94a939f",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e196f5f",
   "metadata": {},
   "source": [
    "# Data loading and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51231c",
   "metadata": {},
   "source": [
    "## Define the merge function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dabc62",
   "metadata": {},
   "source": [
    "\n",
    "The following code block will:\n",
    "- Find all CSV files in the data directory\n",
    "- Read each CSV file into a pandas DataFrame\n",
    "- Combine all DataFrames into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the path to the data directory using Path for cross-platform compatibility\n",
    "data_path = Path('data')\n",
    "\n",
    "# Find all CSV files in the directory and sort them for consistent ordering\n",
    "# glob('*.csv') finds all files ending in .csv\n",
    "csv_files = sorted(data_path.glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise ValueError(f\"No CSV files found in {'data'}\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files to merge...\")\n",
    "\n",
    "# Read all CSV files into a list of DataFrames\n",
    "# We'll store each DataFrame in a list before concatenating\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Reading {csv_file.name}...\")\n",
    "    try:\n",
    "        # pd.read_csv() reads the CSV file and converts it to a DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        # If a file can't be read, print a warning but continue with other files\n",
    "        print(f\"Warning: Could not read {csv_file.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not dataframes:\n",
    "    raise ValueError(\"No dataframes were successfully loaded\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "# ignore_index=True creates a new sequential index (0, 1, 2, ...) instead of keeping original indices\n",
    "print(\"Merging dataframes...\")\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display summary information about the merged DataFrame\n",
    "print(f\"\\nMerged DataFrame shape: {merged_df.shape}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"Total columns: {len(merged_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60615c4d",
   "metadata": {},
   "source": [
    "## Explore the merged data\n",
    "\n",
    "Let's take a look at the structure and content of our merged DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to see what the data looks like\n",
    "# head() shows the first 5 rows by default\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the DataFrame\n",
    "# info() shows column names, data types, and non-null counts\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for numeric columns\n",
    "# describe() provides count, mean, std, min, max, and quartiles\n",
    "merged_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301eb2ea",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5beb564",
   "metadata": {},
   "source": [
    "This section of the notebook builds a classification model to predict whether a Kickstarter project will be \"successful\" or \"failed\" using only features available at project launch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77187ca9",
   "metadata": {},
   "source": [
    "\n",
    "## Techniques and Algorithms Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbace213",
   "metadata": {},
   "source": [
    "\n",
    "1. Random Forest Classifier\n",
    "   - Bagged ensemble of decision trees\n",
    "   - Each tree uses random sample with replacement\n",
    "   - Final prediction is mean score across trees\n",
    "\n",
    "2. GridSearchCV for Hyperparameter Tuning\n",
    "   - Examines all possible combinations of specified hyperparameters\n",
    "   - Uses cross-validation to find optimal parameters\n",
    "\n",
    "3. 5-Fold Cross-Validation\n",
    "   - Splits training data into k parts\n",
    "   - Each fold serves as validation data once\n",
    "\n",
    "4. One-Hot Encoding\n",
    "   - Encodes categorical variables with boolean variables (0/1)\n",
    "\n",
    "5. Classification Metrics\n",
    "   - Confusion Matrix, Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "\n",
    "6. Feature Importance\n",
    "   - Understanding which features are most impactful\n",
    "\n",
    "7. Train-Test Split\n",
    "   - 80/20 split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fe18b",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a818ed3",
   "metadata": {},
   "source": [
    "Only features available at project LAUNCH are used. This ensures the model can make predictions when a project is launched, not after it has already received funding.\n",
    "\n",
    "Excluded FEATURES (NOT available at launch):\n",
    "- `backers_count`: Number of backers (only known after launch)\n",
    "- `pledged`: Amount pledged (only known after launch)\n",
    "- `staff_pick`: Whether Kickstarter staff featured the project (selected after launch)\n",
    "- `percent_funded`: Funding percentage (only known after launch)\n",
    "- `usd_pledged`: USD pledged (only known after launch)\n",
    "- `spotlight`: Spotlight status (assigned after campaign ends)\n",
    "- `state_changed_at`: When state changed (after campaign ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6435e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract parent category from JSON\n",
    "def extract_parent_category(cat_str):\n",
    "    \"\"\"Extract the parent category from the nested JSON category field.\"\"\"\n",
    "    try:\n",
    "        cat_dict = json.loads(cat_str.replace('\"\"', '\"'))\n",
    "        return cat_dict.get('parent_name', 'Unknown')\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "df['parent_category'] = df['category'].apply(extract_parent_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Campaign duration (in days)\n",
    "# Calculated from deadline and launched_at timestamps\n",
    "df['campaign_duration_days'] = (df['deadline'] - df['launched_at']) / (60 * 60 * 24)\n",
    "\n",
    "# 3. Has video (binary: 1 if project has video, 0 otherwise)\n",
    "df['has_video'] = (~df['video'].isna()).astype(int)\n",
    "\n",
    "# 4. Blurb length (character count of project description)\n",
    "df['blurb_length'] = df['blurb'].fillna('').apply(len)\n",
    "\n",
    "# 5. Name length (character count of project name)\n",
    "df['name_length'] = df['name'].fillna('').apply(len)\n",
    "\n",
    "# 6. Goal in USD (standardized funding goal)\n",
    "df['goal_usd'] = df['goal'] * df['static_usd_rate']\n",
    "\n",
    "# 7. Is US-based project\n",
    "df['is_us'] = (df['country'] == 'US').astype(int)\n",
    "\n",
    "# 8. Log transform of goal (to handle right-skewed distribution)\n",
    "df['log_goal_usd'] = np.log1p(df['goal_usd'])\n",
    "\n",
    "# Create target variable (1 = successful, 0 = failed)\n",
    "df['target'] = (df['state'] == 'successful').astype(int)\n",
    "\n",
    "print(\"\\nFeatures Created (All available at launch):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. log_goal_usd:         Log of funding goal in USD\")\n",
    "print(\"2. campaign_duration_days: Length of campaign in days\")\n",
    "print(\"3. blurb_length:         Character count of description\")\n",
    "print(\"4. name_length:          Character count of project name\")\n",
    "print(\"5. has_video:            Whether project has a video (0/1)\")\n",
    "print(\"6. is_us:                Whether US-based project (0/1)\")\n",
    "print(\"7. parent_category:      Main category (one-hot encoded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbbafa",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b55079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "numerical_features = [\n",
    "    'log_goal_usd',           # Log of funding goal\n",
    "    'campaign_duration_days', # Campaign length\n",
    "    'blurb_length',           # Description length\n",
    "    'name_length',            # Project name length\n",
    "]\n",
    "\n",
    "# Binary features\n",
    "binary_features = [\n",
    "    'has_video',    # Has video presentation\n",
    "    'is_us',        # US-based project\n",
    "]\n",
    "\n",
    "# Categorical features - will be one-hot encoded\n",
    "categorical_features = ['parent_category']\n",
    "\n",
    "# Prepare the feature matrix\n",
    "X_num = df[numerical_features].copy()\n",
    "X_bin = df[binary_features].astype(int).copy()\n",
    "X_cat = pd.get_dummies(df[categorical_features], drop_first=True)  # drop_first to avoid multicollinearity\n",
    "\n",
    "# Combine all features\n",
    "X = pd.concat([X_num, X_bin, X_cat], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nFeature Matrix Shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66413c22",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b942ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 80/20 split with stratification to maintain class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class balance in both sets\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(f\"  Successful: {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  Failed:     {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33a010",
   "metadata": {},
   "source": [
    "## Baseline Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9aa1ba",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_cv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"1. Logistic Regression:\")\n",
    "print(f\"   CV AUC: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b69d49",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for Logistic Regression\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "dt_cv_scores = cross_val_score(dt, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"\\n2. Decision Tree (max_depth=5):\")\n",
    "print(f\"   CV AUC: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f66c79",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for Logistic Regression\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"\\n3. Random Forest (n_estimators=100):\")\n",
    "print(f\"   CV AUC: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178739e",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean CV scores for each model\n",
    "models = {\n",
    "    'Logistic Regression': lr_cv_scores.mean(),\n",
    "    'Decision Tree': dt_cv_scores.mean(),\n",
    "    'Random Forest': rf_cv_scores.mean()\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for model_name, score in sorted(models.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model_name:25s}: {score:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model = max(models, key=models.get)\n",
    "best_score = models[best_model]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"BEST MODEL: {best_model}\")\n",
    "print(f\"Best CV AUC Score: {best_score:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e2668",
   "metadata": {},
   "source": [
    "> Random Forest is selected for hyperparameter tuning as it achieved the highest cross-validation ROC-AUC score (0.8599) among all three models. Additionally, Random Forest can capture non-linear relationships in the data through its ensemble of decision trees, making it well-suited for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fe7c6",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning(GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d158d0",
   "metadata": {},
   "source": [
    "Using GridSearchCV for hyperparameter optimization\n",
    "\n",
    "Hyperparameters being tuned:\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "- `min_samples_leaf`: Minimum samples required at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"Total combinations to test: {2 * 4 * 3 * 3} = 72\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Parameters Found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Cross-Validation AUC: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc818f9",
   "metadata": {},
   "source": [
    "## Final Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n1. CONFUSION MATRIX:\")\n",
    "print(\"-\" * 40)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"                    Predicted\")\n",
    "print(f\"                  Failed  Successful\")\n",
    "print(f\"Actual Failed      {cm[0,0]:4d}      {cm[0,1]:4d}\")\n",
    "print(f\"Actual Successful  {cm[1,0]:4d}      {cm[1,1]:4d}\")\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n2. CLASSIFICATION METRICS:\")\n",
    "print(\"-\" * 40)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy:   {accuracy:.4f}  (% of total correctly classified)\")\n",
    "print(f\"Precision:  {precision:.4f}  (% of predicted successes that were correct)\")\n",
    "print(f\"Recall:     {recall:.4f}  (% of actual successes correctly predicted)\")\n",
    "print(f\"F1 Score:   {f1:.4f}  (harmonic mean of precision and recall)\")\n",
    "print(f\"ROC-AUC:    {roc_auc:.4f}  (probability ranking performance)\")\n",
    "\n",
    "print(\"\\n3. CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_pred, target_names=['Failed', 'Successful']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe34d3e",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb218fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (_, row) in enumerate(feature_importance.iterrows(), 1):\n",
    "    print(f\"{rank:2d}. {row['feature']:35s} {row['importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ad334",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8cb8c",
   "metadata": {},
   "source": [
    "### Model Logic Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc0f47",
   "metadata": {},
   "source": [
    "The Random Forest classifier works by building multiple decision trees, each trained on a random subset of the data and features. Each tree makes\n",
    "a prediction, and the final prediction is the majority vote across all trees. This ensemble approach reduces overfitting and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368bf35",
   "metadata": {},
   "source": [
    "### Key Findings From Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814bc88",
   "metadata": {},
   "source": [
    "#### 1. Goal Amount `(log_goal_usd)` - MOST IMPORTANT (~22%)\n",
    "   - Lower funding goals are more likely to succeed\n",
    "   - Very high goals make projects harder to fully fund\n",
    "   - The LOG transformation helps handle the wide range of goal values\n",
    "     (from hundreds to millions of dollars)\n",
    "   - Business insight: Creators should set realistic, achievable goals\n",
    "\n",
    "#### 2. Campagin Duration (`campaign_duration_days`) - SECOND MOST IMPORTANT (~17%)\n",
    "   - Campaign length significantly affects funding success\n",
    "   - Too short campaigns may not allow enough time to build momentum\n",
    "   - Too long campaigns may lose urgency and backer interest\n",
    "   - Business insight: Optimal campaign length balances urgency with reach\n",
    "\n",
    "#### 3. Project Name & Description (`name_length` ~12%, `blurb_length` ~11%)\n",
    "   - Together these account for ~23% of predictive power\n",
    "   - Longer, more descriptive project names attract attention\n",
    "   - Detailed blurbs provide backers with necessary information\n",
    "   - Business insight: Invest time in crafting clear, descriptive copy\n",
    "\n",
    "#### 4. Project Category (`parent_category` - combined ~26%)\n",
    "   - Music projects show the highest category importance (~8%)\n",
    "   - Games and Journalism categories also show notable effects\n",
    "   - Different categories have inherently different success dynamics\n",
    "   - Business insight: Success benchmarks vary by category\n",
    "\n",
    "#### 5. Video Presence (`has_video` ~7%)\n",
    "   - Projects with videos are more likely to succeed\n",
    "   - Videos help communicate vision and build trust with backers\n",
    "   - Business insight: Creating a compelling video is worth the investment\n",
    "\n",
    "#### 6. Geographic Location (`is_us` ~2%)\n",
    "   - US-based projects show slightly different funding patterns\n",
    "   - Relatively minor effect compared to other features\n",
    "   - May reflect payment processing, shipping, or cultural factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdafac",
   "metadata": {},
   "source": [
    "### Actionable Recommendations For Project Creators:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kickstarter-env)",
   "language": "python",
   "name": "kickstarter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
